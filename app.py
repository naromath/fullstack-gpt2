"""
    1. ì´ì „ ê³¼ì œì—ì„œ êµ¬í˜„í•œ RAG íŒŒì´í”„ë¼ì¸ì„ Streamlitìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    2. íŒŒì¼ ì—…ë¡œë“œ ë° ì±„íŒ… ê¸°ë¡ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
    3. ì‚¬ìš©ìê°€ ìì²´ OpenAI API í‚¤ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í—ˆìš©í•˜ê³ , st.sidebar ë‚´ë¶€ì˜ st.inputì—ì„œ ì´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    4. st.sidebarë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ë¦¿ ì•±ì˜ ì½”ë“œì™€ í•¨ê»˜ ê¹ƒí—ˆë¸Œ ë¦¬í¬ì§€í† ë¦¬ì— ë§í¬ë¥¼ ë„£ìŠµë‹ˆë‹¤.
    
    
    a. ì½”ë“œë¥¼ ê³µê°œ Github ë¦¬í¬ì§€í† ë¦¬ì— í‘¸ì‹œí•©ë‹ˆë‹¤.
    b. ë‹¨. OpenAI API í‚¤ë¥¼ Github ë¦¬í¬ì§€í† ë¦¬ì— í‘¸ì‹œí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.
    c. ì—¬ê¸°ì—ì„œ ê³„ì •ì„ ê°œì„¤í•˜ì„¸ìš”: https://share.streamlit.io/
    d. ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”: https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app#deploy-your-app-1
    e. ì•±ì˜ êµ¬ì¡°ê°€ ì•„ë˜ì™€ ê°™ì€ì§€ í™•ì¸í•˜ê³  ë°°í¬ ì–‘ì‹ì˜ Main file path ì— app.pyë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    
"""

from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.base import BaseCallbackHandler

import streamlit as st
import time

class ChatCallbackHandler(BaseCallbackHandler):
    
    message=""
       
    
    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()
                       
    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)
        
    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")
            
                
    
st.set_page_config(
    page_icon="ğŸ–¥ï¸",
    page_title="RAG PIPE"
)

st.title("RAG PIPE")


@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    cashe_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        chunk_size = 600,
        chunk_overlap = 100,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cashed_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cashe_dir)
    vectorstore = FAISS.from_documents(docs, cashed_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)
        
def save_message(message, role):
    st.session_state["message"].append({"message": message, "role" :role})
    

def paint_history():
    for message in st.session_state["message"]:
        send_message(message["message"], message["role"], save=False)
        
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)
        

prompt = ChatPromptTemplate.from_messages(
    [
        ("system","Context: {context}",),
        ("human","{question}"),
    ]
)



st.markdown(
    """
    ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ì‹ ì˜ íŒŒì¼ì„ ë¶„ì„í•´ì£¼ëŠ” ë¬¸ì„œGPT ì…ë‹ˆë‹¤.
            
    ë‹¹ì‹ ì´ ë¶„ì„í•˜ê³  ì‹¶ì€ ë¬¸ì„œë¥¼ ì™¼ìª½ì— ì²¨ë¶€í•´ ì£¼ì„¸ìš”."""
    )

with st.sidebar:
    file = st.file_uploader(
        "Upload a. txt .pdf or .docx file",
        type=["pdf","txt","docx"])



if file:
    retriever = embed_file(file)
    
    

    send_message("ë‹¹ì‹ ì˜ OPENAI API KEYë¥¼ ì™¼ìª½ ì°½ì— ì…ë ¥í•´ ì£¼ì„¸ìš”", "ai", save=False)    
      
    
    key = st.sidebar.text_input("OPENAI API KEY")
    
    if not key:
        send_message("ì•„ì§ API Keyê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.","ai", save=False)
        st.stop()
    else:
        send_message("ì¤€ë¹„ ë¬ì–´ìš”! ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”", "ai", save=False)             
        
    llm=ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.1,
        streaming=True,
        callbacks=[ChatCallbackHandler(),],
        openai_api_key=key
    )
    
    paint_history()
    
    message= st.chat_input("Ask anything about your file...")
    
    if message:
        send_message(message, "human")
        chain = {
            "context": retriever | RunnableLambda(format_docs),
            "question": RunnablePassthrough(),
        } | prompt | llm
        with st.chat_message("ai"):
            response = chain.invoke(message)
        
else:
    st.session_state["message"]=[]
    
    
        
        
        
        
        
